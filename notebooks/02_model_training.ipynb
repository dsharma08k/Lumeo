{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lumeo - Model Training\n",
                "\n",
                "Train a U-Net model for low-light image enhancement on the LOL dataset.\n",
                "\n",
                "**Model:** Heavy U-Net (~20M parameters)  \n",
                "**Optimized for:** Free Colab tier with checkpointing  \n",
                "**Training Time:** ~10-15 hours (across sessions)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (if needed)\n",
                "!pip install -q pytorch-msssim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# UPDATE THIS PATH to your Drive location\n",
                "DATASET_ROOT = '/content/drive/MyDrive/Lumeo/datasets'\n",
                "CHECKPOINT_DIR = '/content/drive/MyDrive/Lumeo/checkpoints'\n",
                "\n",
                "import os\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from torchvision.models import vgg16, VGG16_Weights\n",
                "from pytorch_msssim import ssim, SSIM\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "from pathlib import Path\n",
                "import time\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Set device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LOLDataset(Dataset):\n",
                "    \"\"\"LOL Dataset for low-light image enhancement\"\"\"\n",
                "    \n",
                "    def __init__(self, low_dir, high_dir, img_size=256, augment=True):\n",
                "        self.low_dir = Path(low_dir)\n",
                "        self.high_dir = Path(high_dir)\n",
                "        self.img_size = img_size\n",
                "        self.augment = augment\n",
                "        \n",
                "        # Get matched pairs\n",
                "        low_files = set(f.name for f in self.low_dir.glob('*.png'))\n",
                "        high_files = set(f.name for f in self.high_dir.glob('*.png'))\n",
                "        self.image_names = sorted(list(low_files.intersection(high_files)))\n",
                "        \n",
                "        print(f\"Found {len(self.image_names)} paired images\")\n",
                "        \n",
                "        # Transforms\n",
                "        self.to_tensor = transforms.ToTensor()\n",
                "        self.resize = transforms.Resize((img_size, img_size), antialias=True)\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.image_names)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        name = self.image_names[idx]\n",
                "        \n",
                "        # Load images\n",
                "        low_img = Image.open(self.low_dir / name).convert('RGB')\n",
                "        high_img = Image.open(self.high_dir / name).convert('RGB')\n",
                "        \n",
                "        # Convert to tensor [0, 1]\n",
                "        low_tensor = self.to_tensor(low_img)\n",
                "        high_tensor = self.to_tensor(high_img)\n",
                "        \n",
                "        # Resize\n",
                "        low_tensor = self.resize(low_tensor)\n",
                "        high_tensor = self.resize(high_tensor)\n",
                "        \n",
                "        # Augmentation\n",
                "        if self.augment:\n",
                "            # Random horizontal flip\n",
                "            if torch.rand(1) > 0.5:\n",
                "                low_tensor = torch.flip(low_tensor, [-1])\n",
                "                high_tensor = torch.flip(high_tensor, [-1])\n",
                "            \n",
                "            # Random vertical flip\n",
                "            if torch.rand(1) > 0.5:\n",
                "                low_tensor = torch.flip(low_tensor, [-2])\n",
                "                high_tensor = torch.flip(high_tensor, [-2])\n",
                "        \n",
                "        return low_tensor, high_tensor"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create datasets\n",
                "TRAIN_LOW = os.path.join(DATASET_ROOT, 'LOLdataset/our485/low')\n",
                "TRAIN_HIGH = os.path.join(DATASET_ROOT, 'LOLdataset/our485/high')\n",
                "VAL_LOW = os.path.join(DATASET_ROOT, 'LOLdataset/eval15/low')\n",
                "VAL_HIGH = os.path.join(DATASET_ROOT, 'LOLdataset/eval15/high')\n",
                "\n",
                "train_dataset = LOLDataset(TRAIN_LOW, TRAIN_HIGH, img_size=256, augment=True)\n",
                "val_dataset = LOLDataset(VAL_LOW, VAL_HIGH, img_size=256, augment=False)\n",
                "\n",
                "# DataLoaders (num_workers=0 to avoid Colab multiprocessing issues)\n",
                "BATCH_SIZE = 4  # Reduced for free tier GPU memory\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
                "\n",
                "print(f\"Training batches: {len(train_loader)}\")\n",
                "print(f\"Validation batches: {len(val_loader)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize a batch\n",
                "low_batch, high_batch = next(iter(train_loader))\n",
                "print(f\"Batch shape: {low_batch.shape}\")\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "for i in range(4):\n",
                "    axes[0, i].imshow(low_batch[i].permute(1, 2, 0).numpy())\n",
                "    axes[0, i].set_title('Low-Light')\n",
                "    axes[0, i].axis('off')\n",
                "    \n",
                "    axes[1, i].imshow(high_batch[i].permute(1, 2, 0).numpy())\n",
                "    axes[1, i].set_title('Normal-Light')\n",
                "    axes[1, i].axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. U-Net Model (Heavy ~20M params)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConvBlock(nn.Module):\n",
                "    \"\"\"Double convolution block with BatchNorm and ReLU\"\"\"\n",
                "    def __init__(self, in_ch, out_ch):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
                "            nn.BatchNorm2d(out_ch),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
                "            nn.BatchNorm2d(out_ch),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.conv(x)\n",
                "\n",
                "\n",
                "class EncoderBlock(nn.Module):\n",
                "    \"\"\"Encoder block: ConvBlock + MaxPool\"\"\"\n",
                "    def __init__(self, in_ch, out_ch):\n",
                "        super().__init__()\n",
                "        self.conv = ConvBlock(in_ch, out_ch)\n",
                "        self.pool = nn.MaxPool2d(2)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        skip = self.conv(x)\n",
                "        down = self.pool(skip)\n",
                "        return skip, down\n",
                "\n",
                "\n",
                "class DecoderBlock(nn.Module):\n",
                "    \"\"\"Decoder block: Upsample + Concat + ConvBlock\"\"\"\n",
                "    def __init__(self, in_ch, out_ch):\n",
                "        super().__init__()\n",
                "        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)\n",
                "        self.conv = ConvBlock(in_ch, out_ch)\n",
                "    \n",
                "    def forward(self, x, skip):\n",
                "        x = self.up(x)\n",
                "        x = torch.cat([x, skip], dim=1)\n",
                "        return self.conv(x)\n",
                "\n",
                "\n",
                "class UNet(nn.Module):\n",
                "    \"\"\"Heavy U-Net for image enhancement (~20M params)\"\"\"\n",
                "    def __init__(self, in_channels=3, out_channels=3):\n",
                "        super().__init__()\n",
                "        \n",
                "        # Encoder (heavier channels)\n",
                "        self.enc1 = EncoderBlock(in_channels, 64)\n",
                "        self.enc2 = EncoderBlock(64, 128)\n",
                "        self.enc3 = EncoderBlock(128, 256)\n",
                "        self.enc4 = EncoderBlock(256, 512)\n",
                "        \n",
                "        # Bottleneck\n",
                "        self.bottleneck = ConvBlock(512, 1024)\n",
                "        \n",
                "        # Decoder\n",
                "        self.dec4 = DecoderBlock(1024, 512)\n",
                "        self.dec3 = DecoderBlock(512, 256)\n",
                "        self.dec2 = DecoderBlock(256, 128)\n",
                "        self.dec1 = DecoderBlock(128, 64)\n",
                "        \n",
                "        # Output\n",
                "        self.out_conv = nn.Conv2d(64, out_channels, 1)\n",
                "        \n",
                "        # Initialize weights\n",
                "        self._init_weights()\n",
                "    \n",
                "    def _init_weights(self):\n",
                "        for m in self.modules():\n",
                "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
                "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
                "            elif isinstance(m, nn.BatchNorm2d):\n",
                "                nn.init.constant_(m.weight, 1)\n",
                "                nn.init.constant_(m.bias, 0)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Encoder\n",
                "        skip1, x = self.enc1(x)\n",
                "        skip2, x = self.enc2(x)\n",
                "        skip3, x = self.enc3(x)\n",
                "        skip4, x = self.enc4(x)\n",
                "        \n",
                "        # Bottleneck\n",
                "        x = self.bottleneck(x)\n",
                "        \n",
                "        # Decoder\n",
                "        x = self.dec4(x, skip4)\n",
                "        x = self.dec3(x, skip3)\n",
                "        x = self.dec2(x, skip2)\n",
                "        x = self.dec1(x, skip1)\n",
                "        \n",
                "        # Output with sigmoid for [0, 1] range\n",
                "        return torch.sigmoid(self.out_conv(x))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create model\n",
                "model = UNet().to(device)\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"Total parameters: {total_params:,}\")\n",
                "print(f\"Trainable parameters: {trainable_params:,}\")\n",
                "print(f\"Model size: ~{total_params * 4 / 1e6:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test forward pass\n",
                "test_input = torch.randn(1, 3, 256, 256).to(device)\n",
                "test_output = model(test_input)\n",
                "print(f\"Input shape: {test_input.shape}\")\n",
                "print(f\"Output shape: {test_output.shape}\")\n",
                "print(f\"Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Loss Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PerceptualLoss(nn.Module):\n",
                "    \"\"\"Perceptual loss using VGG16 features\"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].eval()\n",
                "        for param in vgg.parameters():\n",
                "            param.requires_grad = False\n",
                "        self.vgg = vgg\n",
                "        self.criterion = nn.L1Loss()\n",
                "        \n",
                "        # ImageNet normalization\n",
                "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
                "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
                "    \n",
                "    def normalize(self, x):\n",
                "        return (x - self.mean) / self.std\n",
                "    \n",
                "    def forward(self, pred, target):\n",
                "        pred_features = self.vgg(self.normalize(pred))\n",
                "        target_features = self.vgg(self.normalize(target))\n",
                "        return self.criterion(pred_features, target_features)\n",
                "\n",
                "\n",
                "class CombinedLoss(nn.Module):\n",
                "    \"\"\"Combined loss: L1 + Perceptual + SSIM\"\"\"\n",
                "    def __init__(self, l1_weight=1.0, perceptual_weight=0.1, ssim_weight=0.1):\n",
                "        super().__init__()\n",
                "        self.l1 = nn.L1Loss()\n",
                "        self.perceptual = PerceptualLoss()\n",
                "        self.ssim_loss = SSIM(data_range=1.0, size_average=True, channel=3)\n",
                "        \n",
                "        self.l1_weight = l1_weight\n",
                "        self.perceptual_weight = perceptual_weight\n",
                "        self.ssim_weight = ssim_weight\n",
                "    \n",
                "    def forward(self, pred, target):\n",
                "        l1_loss = self.l1(pred, target)\n",
                "        perc_loss = self.perceptual(pred, target)\n",
                "        ssim_val = self.ssim_loss(pred, target)\n",
                "        ssim_loss = 1 - ssim_val  # Convert to loss\n",
                "        \n",
                "        total = (self.l1_weight * l1_loss + \n",
                "                 self.perceptual_weight * perc_loss + \n",
                "                 self.ssim_weight * ssim_loss)\n",
                "        \n",
                "        return total, {\n",
                "            'l1': l1_loss.item(),\n",
                "            'perceptual': perc_loss.item(),\n",
                "            'ssim': ssim_val.item()\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize loss and optimizer\n",
                "criterion = CombinedLoss().to(device)\n",
                "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200, eta_min=1e-6)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop with Checkpointing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_psnr(pred, target):\n",
                "    \"\"\"Compute PSNR in dB\"\"\"\n",
                "    mse = torch.mean((pred - target) ** 2)\n",
                "    if mse == 0:\n",
                "        return float('inf')\n",
                "    return 10 * torch.log10(1.0 / mse).item()\n",
                "\n",
                "\n",
                "def save_checkpoint(model, optimizer, scheduler, epoch, best_psnr, path):\n",
                "    \"\"\"Save training checkpoint to Drive\"\"\"\n",
                "    torch.save({\n",
                "        'epoch': epoch,\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'optimizer_state_dict': optimizer.state_dict(),\n",
                "        'scheduler_state_dict': scheduler.state_dict(),\n",
                "        'best_psnr': best_psnr\n",
                "    }, path)\n",
                "    print(f\"Checkpoint saved: {path}\")\n",
                "\n",
                "\n",
                "def load_checkpoint(path, model, optimizer=None, scheduler=None):\n",
                "    \"\"\"Load checkpoint from Drive\"\"\"\n",
                "    checkpoint = torch.load(path, map_location=device)\n",
                "    model.load_state_dict(checkpoint['model_state_dict'])\n",
                "    if optimizer:\n",
                "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
                "    if scheduler:\n",
                "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
                "    return checkpoint['epoch'], checkpoint['best_psnr']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, loader, criterion, optimizer):\n",
                "    \"\"\"Train for one epoch\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    metrics = {'l1': 0, 'perceptual': 0, 'ssim': 0}\n",
                "    \n",
                "    pbar = tqdm(loader, desc='Training')\n",
                "    for low, high in pbar:\n",
                "        low, high = low.to(device), high.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        output = model(low)\n",
                "        loss, batch_metrics = criterion(output, high)\n",
                "        \n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        for k in metrics:\n",
                "            metrics[k] += batch_metrics[k]\n",
                "        \n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    n = len(loader)\n",
                "    return total_loss / n, {k: v / n for k, v in metrics.items()}\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def validate(model, loader, criterion):\n",
                "    \"\"\"Validate model\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    total_psnr = 0\n",
                "    total_ssim = 0\n",
                "    \n",
                "    for low, high in loader:\n",
                "        low, high = low.to(device), high.to(device)\n",
                "        output = model(low)\n",
                "        loss, metrics = criterion(output, high)\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        total_psnr += compute_psnr(output, high)\n",
                "        total_ssim += metrics['ssim']\n",
                "    \n",
                "    n = len(loader)\n",
                "    return total_loss / n, total_psnr / n, total_ssim / n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training configuration\n",
                "NUM_EPOCHS = 200\n",
                "SAVE_EVERY = 10  # Save checkpoint every N epochs\n",
                "RESUME = False   # Set True to resume from checkpoint\n",
                "\n",
                "# Paths\n",
                "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'latest_checkpoint.pth')\n",
                "best_model_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
                "\n",
                "# Resume if needed\n",
                "start_epoch = 0\n",
                "best_psnr = 0\n",
                "\n",
                "if RESUME and os.path.exists(checkpoint_path):\n",
                "    start_epoch, best_psnr = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
                "    start_epoch += 1\n",
                "    print(f\"Resuming from epoch {start_epoch}, best PSNR: {best_psnr:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "history = {'train_loss': [], 'val_loss': [], 'psnr': [], 'ssim': []}\n",
                "\n",
                "print(f\"Starting training from epoch {start_epoch}...\")\n",
                "print(f\"Training on {len(train_dataset)} images, validating on {len(val_dataset)} images\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for epoch in range(start_epoch, NUM_EPOCHS):\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Train\n",
                "    train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer)\n",
                "    \n",
                "    # Validate\n",
                "    val_loss, val_psnr, val_ssim = validate(model, val_loader, criterion)\n",
                "    \n",
                "    # Update scheduler\n",
                "    scheduler.step()\n",
                "    \n",
                "    # Log\n",
                "    history['train_loss'].append(train_loss)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['psnr'].append(val_psnr)\n",
                "    history['ssim'].append(val_ssim)\n",
                "    \n",
                "    epoch_time = time.time() - start_time\n",
                "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} ({epoch_time:.1f}s) | \"\n",
                "          f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
                "          f\"PSNR: {val_psnr:.2f} dB | SSIM: {val_ssim:.4f}\")\n",
                "    \n",
                "    # Save best model\n",
                "    if val_psnr > best_psnr:\n",
                "        best_psnr = val_psnr\n",
                "        torch.save(model.state_dict(), best_model_path)\n",
                "        print(f\"  --> New best model saved! PSNR: {best_psnr:.2f}\")\n",
                "    \n",
                "    # Save checkpoint for resuming\n",
                "    if (epoch + 1) % SAVE_EVERY == 0:\n",
                "        save_checkpoint(model, optimizer, scheduler, epoch, best_psnr, checkpoint_path)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(f\"Training complete! Best PSNR: {best_psnr:.2f} dB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "axes[0].plot(history['train_loss'], label='Train')\n",
                "axes[0].plot(history['val_loss'], label='Val')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].set_title('Loss')\n",
                "axes[0].legend()\n",
                "\n",
                "axes[1].plot(history['psnr'])\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('PSNR (dB)')\n",
                "axes[1].set_title('Validation PSNR')\n",
                "\n",
                "axes[2].plot(history['ssim'])\n",
                "axes[2].set_xlabel('Epoch')\n",
                "axes[2].set_ylabel('SSIM')\n",
                "axes[2].set_title('Validation SSIM')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_curves.png'), dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Qualitative Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load best model\n",
                "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
                "model.eval()\n",
                "\n",
                "@torch.no_grad()\n",
                "def enhance_image(model, img_tensor):\n",
                "    \"\"\"Enhance a single image\"\"\"\n",
                "    return model(img_tensor.unsqueeze(0).to(device)).squeeze(0).cpu()\n",
                "\n",
                "# Visualize results on validation set\n",
                "fig, axes = plt.subplots(5, 3, figsize=(12, 20))\n",
                "\n",
                "for idx in range(5):\n",
                "    low, high = val_dataset[idx]\n",
                "    enhanced = enhance_image(model, low)\n",
                "    \n",
                "    axes[idx, 0].imshow(low.permute(1, 2, 0).numpy())\n",
                "    axes[idx, 0].set_title('Input (Low-Light)')\n",
                "    axes[idx, 0].axis('off')\n",
                "    \n",
                "    axes[idx, 1].imshow(enhanced.permute(1, 2, 0).numpy())\n",
                "    axes[idx, 1].set_title('Enhanced')\n",
                "    axes[idx, 1].axis('off')\n",
                "    \n",
                "    axes[idx, 2].imshow(high.permute(1, 2, 0).numpy())\n",
                "    axes[idx, 2].set_title('Ground Truth')\n",
                "    axes[idx, 2].axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(CHECKPOINT_DIR, 'validation_results.png'), dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export Model for Backend"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model weights only (for deployment)\n",
                "export_path = os.path.join(CHECKPOINT_DIR, 'lumeo_unet.pth')\n",
                "torch.save(model.state_dict(), export_path)\n",
                "print(f\"Model exported to: {export_path}\")\n",
                "print(f\"Model size: {os.path.getsize(export_path) / 1e6:.1f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Best PSNR: {best_psnr:.2f} dB\")\n",
                "print(f\"Model saved to: {export_path}\")\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Download lumeo_unet.pth from Drive\")\n",
                "print(\"2. Use it in FastAPI backend for inference\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}